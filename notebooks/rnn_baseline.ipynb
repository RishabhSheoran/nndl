{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe10f9b-960c-4d4d-913a-8e238ae2725f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T01:49:57.624077Z",
     "iopub.status.busy": "2021-11-17T01:49:57.623767Z",
     "iopub.status.idle": "2021-11-17T01:50:00.107385Z",
     "shell.execute_reply": "2021-11-17T01:50:00.106259Z",
     "shell.execute_reply.started": "2021-11-17T01:49:57.624006Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from einops import rearrange # ! pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "192f9e83-3086-4fd4-8286-418e55a64732",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T01:50:00.109078Z",
     "iopub.status.busy": "2021-11-17T01:50:00.108700Z",
     "iopub.status.idle": "2021-11-17T01:50:00.239059Z",
     "shell.execute_reply": "2021-11-17T01:50:00.238270Z",
     "shell.execute_reply.started": "2021-11-17T01:50:00.109046Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, train, test):\n",
    "\n",
    "        self.csv = csv\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.all_image_names = self.csv[:][\"filename\"]\n",
    "        self.all_labels = np.array(self.csv.drop([\"filename\", \"ind\"], axis=1))\n",
    "        self.train_ratio = int(0.85 * len(self.csv))\n",
    "        self.valid_ratio = len(self.csv) - self.train_ratio\n",
    "\n",
    "        # set the training data images and labels\n",
    "        if self.train == True:\n",
    "            print(f\"Number of training images: {self.train_ratio}\")\n",
    "            self.image_names = list(self.all_image_names[: self.train_ratio])\n",
    "            self.labels = list(self.all_labels[: self.train_ratio])\n",
    "            # define the training transforms\n",
    "            self.transform = transforms.Compose([transforms.ToTensor(),])\n",
    "\n",
    "        # set the validation data images and labels\n",
    "        elif self.train == False and self.test == False:\n",
    "            print(f\"Number of validation images: {self.valid_ratio}\")\n",
    "            self.image_names = list(self.all_image_names[-self.valid_ratio : -10])\n",
    "            self.labels = list(self.all_labels[-self.valid_ratio :])\n",
    "            # define the validation transforms\n",
    "            self.transform = transforms.Compose([transforms.ToTensor(),])\n",
    "\n",
    "        # set the test data images and labels, only last 10 images\n",
    "        # this, we will use in a separate inference script\n",
    "        elif self.test == True and self.train == False:\n",
    "            self.image_names = list(self.all_image_names[-10:])\n",
    "            self.labels = list(self.all_labels[-10:])\n",
    "            # define the test transforms\n",
    "            self.transform = transforms.Compose([transforms.ToTensor(),])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = cv2.imread(f\"../data/raw_images/{self.image_names[index]}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # apply image transforms\n",
    "        image = self.transform(image)\n",
    "        targets = self.labels[index]\n",
    "\n",
    "        return {\n",
    "            \"image\": torch.tensor(image, dtype=torch.float32),\n",
    "            \"label\": torch.tensor(targets, dtype=torch.float32),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38cef678-1fcd-4b0e-b518-0900a4ed762b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T02:02:30.453628Z",
     "iopub.status.busy": "2021-11-17T02:02:30.453298Z",
     "iopub.status.idle": "2021-11-17T02:02:30.470912Z",
     "shell.execute_reply": "2021-11-17T02:02:30.470022Z",
     "shell.execute_reply.started": "2021-11-17T02:02:30.453596Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used in creating data loaders as well as in train loops\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61874ff8-7c65-44e2-a2b0-ff74b71cff05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T01:50:00.255047Z",
     "iopub.status.busy": "2021-11-17T01:50:00.254730Z",
     "iopub.status.idle": "2021-11-17T01:50:00.274051Z",
     "shell.execute_reply": "2021-11-17T01:50:00.273335Z",
     "shell.execute_reply.started": "2021-11-17T01:50:00.255017Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 849\n",
      "Number of validation images: 150\n"
     ]
    }
   ],
   "source": [
    "train_csv = pd.read_csv(\"../data/processed/train.csv\")\n",
    "# train dataset\n",
    "train_data = ImageDataset(train_csv[:1000], train=True, test=False)\n",
    "# validation dataset\n",
    "valid_data = ImageDataset(train_csv[:1000], train=False, test=False)\n",
    "# train data loader\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# validation data loader\n",
    "valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be08b442-a101-4a8d-b187-5c6158c1bf12",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0a79695-89e4-416b-add7-10d548fbc9fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T01:50:00.498335Z",
     "iopub.status.busy": "2021-11-17T01:50:00.497782Z",
     "iopub.status.idle": "2021-11-17T01:50:00.512432Z",
     "shell.execute_reply": "2021-11-17T01:50:00.511720Z",
     "shell.execute_reply.started": "2021-11-17T01:50:00.498309Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_STEPS = 144\n",
    "N_INPUTS = 768\n",
    "N_NEURONS = 150\n",
    "N_OUTPUTS = 6\n",
    "N_EPHOCS = 10\n",
    "\n",
    "\n",
    "LR = 0.001\n",
    "PATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4a9dfe1-3c81-4605-b62a-7317ef7db3b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T01:50:00.514289Z",
     "iopub.status.busy": "2021-11-17T01:50:00.513653Z",
     "iopub.status.idle": "2021-11-17T01:50:00.531107Z",
     "shell.execute_reply": "2021-11-17T01:50:00.530238Z",
     "shell.execute_reply.started": "2021-11-17T01:50:00.514265Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageRNN(nn.Module):\n",
    "    def __init__(self, batch_size, n_steps, n_inputs, n_neurons, n_outputs):\n",
    "        super(ImageRNN, self).__init__()\n",
    "\n",
    "        self.n_neurons = n_neurons\n",
    "        self.batch_size = batch_size\n",
    "        self.n_steps = n_steps\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "\n",
    "        self.basic_rnn = nn.RNN(self.n_inputs, self.n_neurons)\n",
    "\n",
    "        self.FC = nn.Linear(self.n_neurons, self.n_outputs)\n",
    "\n",
    "    def init_hidden(self,):\n",
    "        # (num_layers, batch_size, n_neurons)\n",
    "        return torch.zeros(1, self.batch_size, self.n_neurons)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # transforms X to dimensions: n_steps X batch_size X n_inputs\n",
    "        X = X.permute(1, 0, 2)\n",
    "\n",
    "        self.batch_size = X.size(1)\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "        lstm_out, self.hidden = self.basic_rnn(X, self.hidden)\n",
    "        out = self.FC(self.hidden)\n",
    "\n",
    "        return out.view(-1, self.n_outputs)  # batch_size X n_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a893e43-4c2b-4409-91d5-757f8444b8c4",
   "metadata": {},
   "source": [
    "### Check if inference works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c671b0c2-1bcf-4e9a-9016-21f312f94664",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T01:50:24.573969Z",
     "iopub.status.busy": "2021-11-17T01:50:24.573707Z",
     "iopub.status.idle": "2021-11-17T01:50:24.607827Z",
     "shell.execute_reply": "2021-11-17T01:50:24.606686Z",
     "shell.execute_reply.started": "2021-11-17T01:50:24.573945Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0302, -0.4416, -0.3831,  0.0461,  0.1974, -0.1136]],\n",
      "       grad_fn=<ViewBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/scjvrz5d2pz78lkds9g31f7m0000gn/T/ipykernel_11783/2345743904.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"image\": torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.4925, 0.3914, 0.4054, 0.5115, 0.5492, 0.4716]],\n",
       "       grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next().values()\n",
    "model = ImageRNN(BATCH_SIZE, N_STEPS, N_INPUTS, N_NEURONS, N_OUTPUTS)\n",
    "img_patches = rearrange(\n",
    "    images.view(1, 3, 144, 256),\n",
    "    \"b c (patch_x x) (patch_y y) -> b (x y) (patch_x patch_y c)\",\n",
    "    patch_x=PATCH_SIZE,\n",
    "    patch_y=PATCH_SIZE,\n",
    ")\n",
    "logits = model(img_patches)\n",
    "print(logits)\n",
    "torch.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718ea345-911c-488f-99c5-d103f2e93d93",
   "metadata": {},
   "source": [
    "### Model instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d90a1757-10d2-4045-bfd1-60d89f651a42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T02:01:21.298910Z",
     "iopub.status.busy": "2021-11-17T02:01:21.298618Z",
     "iopub.status.idle": "2021-11-17T02:01:21.323460Z",
     "shell.execute_reply": "2021-11-17T02:01:21.322541Z",
     "shell.execute_reply.started": "2021-11-17T02:01:21.298882Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model instance\n",
    "model = ImageRNN(BATCH_SIZE, N_STEPS, N_INPUTS, N_NEURONS, N_OUTPUTS)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "def get_accuracy(logit, target, batch_size):\n",
    "    \"\"\" Obtain accuracy for training round \"\"\"\n",
    "    # FIXME: The below does not account for batch_size > 1 - change the definition accordingly\n",
    "\n",
    "    pred = logit > 0.5\n",
    "    actual = target == 1\n",
    "    acc = ((pred & actual).sum().item() / target.shape[1]) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580e48b8-efd0-4216-998b-b3fd2bb02e21",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d632b89-0a81-46a1-82ee-a93f4e841a08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T01:50:38.999334Z",
     "iopub.status.busy": "2021-11-17T01:50:38.999046Z",
     "iopub.status.idle": "2021-11-17T01:54:17.943314Z",
     "shell.execute_reply": "2021-11-17T01:54:17.942252Z",
     "shell.execute_reply.started": "2021-11-17T01:50:38.999309Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/scjvrz5d2pz78lkds9g31f7m0000gn/T/ipykernel_11783/2345743904.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"image\": torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | Loss: 0.3985 | Train Accuracy: 7.53\n",
      "Epoch:  1 | Loss: 0.2935 | Train Accuracy: 13.50\n",
      "Epoch:  2 | Loss: 0.2391 | Train Accuracy: 15.29\n",
      "Epoch:  3 | Loss: 0.1991 | Train Accuracy: 16.71\n",
      "Epoch:  4 | Loss: 0.1688 | Train Accuracy: 17.30\n",
      "Epoch:  5 | Loss: 0.1482 | Train Accuracy: 17.96\n",
      "Epoch:  6 | Loss: 0.1313 | Train Accuracy: 18.30\n",
      "Epoch:  7 | Loss: 0.1436 | Train Accuracy: 17.85\n",
      "Epoch:  8 | Loss: 0.1174 | Train Accuracy: 18.71\n",
      "Epoch:  9 | Loss: 0.1139 | Train Accuracy: 18.87\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPHOCS):  # loop over the dataset multiple times\n",
    "    train_running_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    model.train()\n",
    "\n",
    "    # TRAINING ROUND\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # reset hidden states\n",
    "        model.hidden = model.init_hidden()\n",
    "\n",
    "        # get the inputs\n",
    "        inputs, labels = data.values()\n",
    "        img_patches = rearrange(\n",
    "            inputs.view(BATCH_SIZE, 3, 144, 256),\n",
    "            \"b c (patch_x x) (patch_y y) -> b (x y) (patch_x patch_y c)\",\n",
    "            patch_x=PATCH_SIZE,\n",
    "            patch_y=PATCH_SIZE,\n",
    "        )\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(img_patches)\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_running_loss += loss.detach().item()\n",
    "        train_acc += get_accuracy(outputs, labels, BATCH_SIZE)\n",
    "\n",
    "    model.eval()\n",
    "    print(\n",
    "        \"Epoch:  %d | Loss: %.4f | Train Accuracy: %.2f\"\n",
    "        % (epoch, train_running_loss / i, train_acc / i)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2130c8d0-f222-460f-ac90-a349ec7821b2",
   "metadata": {},
   "source": [
    "### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a279e1a-cb25-4280-b3af-5ab5553cc4da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T01:58:57.869156Z",
     "iopub.status.busy": "2021-11-17T01:58:57.868893Z",
     "iopub.status.idle": "2021-11-17T01:58:58.802223Z",
     "shell.execute_reply": "2021-11-17T01:58:58.801273Z",
     "shell.execute_reply.started": "2021-11-17T01:58:57.869131Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k7/scjvrz5d2pz78lkds9g31f7m0000gn/T/ipykernel_11783/2345743904.py:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \"image\": torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 2.16\n"
     ]
    }
   ],
   "source": [
    "test_acc = 0.0\n",
    "for i, data in enumerate(valid_loader, 0):\n",
    "    inputs, labels = data.values()\n",
    "    img_patches = rearrange(\n",
    "        inputs.view(BATCH_SIZE, 3, 144, 256),\n",
    "        \"b c (patch_x x) (patch_y y) -> b (x y) (patch_x patch_y c)\",\n",
    "        patch_x=PATCH_SIZE,\n",
    "        patch_y=PATCH_SIZE,\n",
    "    )\n",
    "\n",
    "    outputs = model(img_patches)\n",
    "\n",
    "    test_acc += get_accuracy(outputs, labels, BATCH_SIZE)\n",
    "\n",
    "print(\"Test Accuracy: %.2f\" % (test_acc / i))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9a6de5-7f3b-48a4-baa5-2198d0a4121e",
   "metadata": {},
   "source": [
    "### Sample image patch generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b364a96-d068-4f39-b9cb-57708d58a1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install einops\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# functions to show an image\n",
    "# def imshow(img):\n",
    "#     # img = img / 2 + 0.5     # unnormalize\n",
    "#     npimg = img.numpy()\n",
    "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "\n",
    "# get some random training images\n",
    "# dataiter = iter(train_loader)\n",
    "# images, labels = dataiter.next().values()\n",
    "# imshow(images[0])\n",
    "# p = 16\n",
    "# img_patches = rearrange(\n",
    "#     images[0].view(1, 3, 144, 256),\n",
    "#     \"b c (patch_x x) (patch_y y) -> b (x y) (patch_x patch_y c)\",\n",
    "#     patch_x=p,\n",
    "#     patch_y=p,\n",
    "# )\n",
    "# img_patches.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
