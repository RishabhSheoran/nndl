{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe10f9b-960c-4d4d-913a-8e238ae2725f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T16:28:35.906763Z",
     "iopub.status.busy": "2021-11-17T16:28:35.906468Z",
     "iopub.status.idle": "2021-11-17T16:28:38.441247Z",
     "shell.execute_reply": "2021-11-17T16:28:38.440254Z",
     "shell.execute_reply.started": "2021-11-17T16:28:35.906698Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from einops import rearrange  # ! pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "192f9e83-3086-4fd4-8286-418e55a64732",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T16:28:38.442596Z",
     "iopub.status.busy": "2021-11-17T16:28:38.442331Z",
     "iopub.status.idle": "2021-11-17T16:28:38.566866Z",
     "shell.execute_reply": "2021-11-17T16:28:38.566171Z",
     "shell.execute_reply.started": "2021-11-17T16:28:38.442572Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, csv, train, test):\n",
    "\n",
    "        self.csv = csv\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.all_image_names = self.csv[:][\"filename\"]\n",
    "        self.all_labels = np.array(self.csv.drop([\"filename\", \"ind\"], axis=1))\n",
    "        self.train_ratio = int(0.85 * len(self.csv))\n",
    "        self.valid_ratio = len(self.csv) - self.train_ratio\n",
    "\n",
    "        # set the training data images and labels\n",
    "        if self.train == True:\n",
    "            print(f\"Number of training images: {self.train_ratio}\")\n",
    "            self.image_names = list(self.all_image_names[: self.train_ratio])\n",
    "            self.labels = list(self.all_labels[: self.train_ratio])\n",
    "            # define the training transforms\n",
    "            self.transform = transforms.Compose([transforms.ToTensor(),])\n",
    "\n",
    "        # set the validation data images and labels\n",
    "        elif self.train == False and self.test == False:\n",
    "            print(f\"Number of validation images: {self.valid_ratio}\")\n",
    "            self.image_names = list(self.all_image_names[-self.valid_ratio : -10])\n",
    "            self.labels = list(self.all_labels[-self.valid_ratio :])\n",
    "            # define the validation transforms\n",
    "            self.transform = transforms.Compose([transforms.ToTensor(),])\n",
    "\n",
    "        # set the test data images and labels, only last 10 images\n",
    "        # this, we will use in a separate inference script\n",
    "        elif self.test == True and self.train == False:\n",
    "            self.image_names = list(self.all_image_names[-10:])\n",
    "            self.labels = list(self.all_labels[-10:])\n",
    "            # define the test transforms\n",
    "            self.transform = transforms.Compose([transforms.ToTensor(),])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = cv2.imread(f\"../data/raw_images/{self.image_names[index]}\")\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # apply image transforms\n",
    "        image = self.transform(image)\n",
    "        targets = self.labels[index]\n",
    "\n",
    "        return {\n",
    "            \"image\": torch.tensor(image, dtype=torch.float32),\n",
    "            \"label\": torch.tensor(targets, dtype=torch.float32),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38cef678-1fcd-4b0e-b518-0900a4ed762b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T16:28:38.570047Z",
     "iopub.status.busy": "2021-11-17T16:28:38.569692Z",
     "iopub.status.idle": "2021-11-17T16:28:38.581248Z",
     "shell.execute_reply": "2021-11-17T16:28:38.580323Z",
     "shell.execute_reply.started": "2021-11-17T16:28:38.570017Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Used in creating data loaders as well as in train loops\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61874ff8-7c65-44e2-a2b0-ff74b71cff05",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T16:28:38.582514Z",
     "iopub.status.busy": "2021-11-17T16:28:38.582217Z",
     "iopub.status.idle": "2021-11-17T16:28:38.602083Z",
     "shell.execute_reply": "2021-11-17T16:28:38.601046Z",
     "shell.execute_reply.started": "2021-11-17T16:28:38.582489Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 850\n",
      "Number of validation images: 150\n"
     ]
    }
   ],
   "source": [
    "train_csv = pd.read_csv(\"../data/processed/train.csv\")\n",
    "# train dataset\n",
    "train_data = ImageDataset(train_csv[:1000], train=True, test=False)\n",
    "# validation dataset\n",
    "valid_data = ImageDataset(train_csv[:1000], train=False, test=False)\n",
    "# train data loader\n",
    "train_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\n",
    "# validation data loader\n",
    "valid_loader = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be08b442-a101-4a8d-b187-5c6158c1bf12",
   "metadata": {},
   "source": [
    "### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0a79695-89e4-416b-add7-10d548fbc9fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T17:12:57.168032Z",
     "iopub.status.busy": "2021-11-17T17:12:57.166931Z",
     "iopub.status.idle": "2021-11-17T17:12:57.187737Z",
     "shell.execute_reply": "2021-11-17T17:12:57.186493Z",
     "shell.execute_reply.started": "2021-11-17T17:12:57.167981Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_EPHOCS = 50\n",
    "\n",
    "\n",
    "LR = 0.001\n",
    "PATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9db200f2-b2b3-4435-b8bc-b66938d3b8cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T16:28:38.615954Z",
     "iopub.status.busy": "2021-11-17T16:28:38.615522Z",
     "iopub.status.idle": "2021-11-17T16:28:38.644420Z",
     "shell.execute_reply": "2021-11-17T16:28:38.643750Z",
     "shell.execute_reply.started": "2021-11-17T16:28:38.615917Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Transformer_encoder(nn.Module):\n",
    "    def __init__(self, hidden_size, nb_heads):\n",
    "        super(Transformer_encoder, self).__init__()\n",
    "        assert hidden_size == nb_heads * (\n",
    "            hidden_size // nb_heads\n",
    "        )  # check if hidden_size is divisible by nb_heads\n",
    "        self.MHA = nn.MultiheadAttention(hidden_size, nb_heads)\n",
    "        self.LLcat = nn.Linear(2 * hidden_size, hidden_size)\n",
    "        self.LL1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.LL2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.LN1 = nn.LayerNorm(hidden_size)\n",
    "        self.LN2 = nn.LayerNorm(hidden_size)\n",
    "\n",
    "    def forward(self, h_cat, pos=None):\n",
    "        #         seq_length = g_seq.size(0)\n",
    "        #         bs = g_seq.size(1)\n",
    "        #         pos = pos.unsqueeze(dim=1).repeat_interleave(\n",
    "        #             bs, dim=1\n",
    "        #         )  # size=(seq_length, bs, hidden_dim)\n",
    "        #         h_cat = self.LLcat(\n",
    "        #             torch.cat((g_seq, pos), dim=2)\n",
    "        #         )  # size=(seq_length, bs, hidden_dim)\n",
    "\n",
    "        h_MHA_seq, _ = self.MHA(\n",
    "            h_cat, h_cat, h_cat\n",
    "        )  # size=(seq_length, bs, hidden_dim)\n",
    "        h = self.LN1(h_cat + h_MHA_seq)  # size=(1, bs, hidden_dim) 2\n",
    "        h_MLP = self.LL2(torch.relu(self.LL1(h)))  # size=(1, bs, hidden_dim)\n",
    "        h_seq = self.LN2(h + h_MLP)  # size=(1, bs, hidden_dim)\n",
    "        return h_seq\n",
    "\n",
    "\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, hidden_size, nb_heads):\n",
    "        super(ANN, self).__init__()\n",
    "        self.encoder = Transformer_encoder(hidden_size, nb_heads)\n",
    "\n",
    "    def forward(self, g_seq, pos=None):\n",
    "        h_enc_seq = self.encoder(g_seq, pos)  # size=(seq_length, bs, hidden_dim)\n",
    "        return h_enc_seq\n",
    "\n",
    "\n",
    "class attention_net(nn.Module):\n",
    "    def __init__(self, hidden_size, nb_heads, no_classes):\n",
    "        super(attention_net, self).__init__()\n",
    "\n",
    "        self.emb_dropout = nn.Dropout(0)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, hidden_size))\n",
    "        self.pos_emb1D = nn.Parameter(torch.randn(144 + 1, hidden_size))\n",
    "        self.mlp_head = nn.Linear(hidden_size, no_classes)\n",
    "\n",
    "        self.learnable_patches = nn.Linear(hidden_size, hidden_size)\n",
    "        self.layer2 = ANN(hidden_size, nb_heads)\n",
    "        self.layer3 = nn.Linear(hidden_size, no_classes)\n",
    "\n",
    "    def expand_cls_to_batch(self, batch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch: batch size\n",
    "        Returns: cls token expanded to the batch size\n",
    "        \"\"\"\n",
    "        return self.cls_token.expand([batch, -1, -1])\n",
    "\n",
    "    def forward(self, img, pos=None):\n",
    "        bs = img.shape[0]\n",
    "        img_patches = rearrange(\n",
    "            img,\n",
    "            \"b c (patch_x x) (patch_y y) -> b (x y) (patch_x patch_y c)\",\n",
    "            patch_x=PATCH_SIZE,\n",
    "            patch_y=PATCH_SIZE,\n",
    "        )\n",
    "\n",
    "        # positional embedding\n",
    "        img_patches = self.learnable_patches(img_patches)\n",
    "        img_patches = torch.cat((self.expand_cls_to_batch(bs), img_patches), dim=1)\n",
    "\n",
    "        patch_embeddings = self.emb_dropout(img_patches + self.pos_emb1D)\n",
    "\n",
    "        h_seq = self.layer2(patch_embeddings, pos)  # size=(seq_length, bs, hidden_dim)\n",
    "        score_seq = self.layer3(h_seq[:, 0, :])  # size=(seq_length, bs, vocab_size)\n",
    "        return score_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a893e43-4c2b-4409-91d5-757f8444b8c4",
   "metadata": {},
   "source": [
    "### Check if inference works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39d533b4-70a8-4e2f-89e8-948d7757cec2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T16:28:38.645898Z",
     "iopub.status.busy": "2021-11-17T16:28:38.645504Z",
     "iopub.status.idle": "2021-11-17T16:28:38.705565Z",
     "shell.execute_reply": "2021-11-17T16:28:38.704871Z",
     "shell.execute_reply.started": "2021-11-17T16:28:38.645842Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attention_net(\n",
       "  (emb_dropout): Dropout(p=0, inplace=False)\n",
       "  (mlp_head): Linear(in_features=768, out_features=6, bias=True)\n",
       "  (learnable_patches): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (layer2): ANN(\n",
       "    (encoder): Transformer_encoder(\n",
       "      (MHA): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (LLcat): Linear(in_features=1536, out_features=768, bias=True)\n",
       "      (LL1): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LL2): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (LN1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (LN2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_heads = 16\n",
    "hidden_size = 768\n",
    "no_classes = 6\n",
    "net = attention_net(hidden_size, nb_heads, no_classes)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c671b0c2-1bcf-4e9a-9016-21f312f94664",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T16:28:38.706873Z",
     "iopub.status.busy": "2021-11-17T16:28:38.706636Z",
     "iopub.status.idle": "2021-11-17T16:28:38.742725Z",
     "shell.execute_reply": "2021-11-17T16:28:38.741933Z",
     "shell.execute_reply.started": "2021-11-17T16:28:38.706850Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1850, -0.8191,  0.5299, -0.1271,  0.7203, -0.7072]],\n",
      "       grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rishabh\\anaconda3\\envs\\deeplearn_course\\lib\\site-packages\\ipykernel_launcher.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.5461, 0.3059, 0.6295, 0.4683, 0.6727, 0.3302]],\n",
       "       grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next().values()\n",
    "seq_length = 144\n",
    "scores = net(images.view(1, 3, 144, 256))\n",
    "scores.shape\n",
    "print(scores)\n",
    "scores = torch.sigmoid(scores)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718ea345-911c-488f-99c5-d103f2e93d93",
   "metadata": {},
   "source": [
    "### Model instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d90a1757-10d2-4045-bfd1-60d89f651a42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T16:48:35.125289Z",
     "iopub.status.busy": "2021-11-17T16:48:35.125011Z",
     "iopub.status.idle": "2021-11-17T16:48:35.182552Z",
     "shell.execute_reply": "2021-11-17T16:48:35.181902Z",
     "shell.execute_reply.started": "2021-11-17T16:48:35.125264Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model instance\n",
    "model = attention_net(hidden_size, nb_heads, no_classes)\n",
    "model = model.to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "def get_error(outputs, labels, batch_size):\n",
    "    lab = torch.where(outputs >= 0.5, 1, 0)\n",
    "    indicator = torch.where(lab != labels, 1, 0)\n",
    "    non_matches = torch.sum(indicator, axis=0)\n",
    "    error = non_matches.float() / batch_size\n",
    "    error = error.sum() / 6\n",
    "    return error.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580e48b8-efd0-4216-998b-b3fd2bb02e21",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d632b89-0a81-46a1-82ee-a93f4e841a08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T17:07:09.092444Z",
     "iopub.status.busy": "2021-11-17T17:07:09.092176Z",
     "iopub.status.idle": "2021-11-17T17:12:57.164192Z",
     "shell.execute_reply": "2021-11-17T17:12:57.162968Z",
     "shell.execute_reply.started": "2021-11-17T17:07:09.092419Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rishabh\\anaconda3\\envs\\deeplearn_course\\lib\\site-packages\\ipykernel_launcher.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | Loss: 0.4931 | Train Error: 0.2348\n",
      "Epoch:  1 | Loss: 0.4563 | Train Error: 0.2236\n",
      "Epoch:  2 | Loss: 0.4504 | Train Error: 0.2197\n",
      "Epoch:  3 | Loss: 0.4481 | Train Error: 0.2216\n",
      "Epoch:  4 | Loss: 0.4492 | Train Error: 0.2173\n",
      "Epoch:  5 | Loss: 0.4456 | Train Error: 0.2187\n",
      "Epoch:  6 | Loss: 0.4458 | Train Error: 0.2173\n",
      "Epoch:  7 | Loss: 0.4452 | Train Error: 0.2152\n",
      "Epoch:  8 | Loss: 0.4455 | Train Error: 0.2173\n",
      "Epoch:  9 | Loss: 0.4431 | Train Error: 0.2155\n",
      "Epoch:  10 | Loss: 0.4438 | Train Error: 0.2132\n",
      "Epoch:  11 | Loss: 0.4458 | Train Error: 0.2167\n",
      "Epoch:  12 | Loss: 0.4434 | Train Error: 0.2140\n",
      "Epoch:  13 | Loss: 0.4435 | Train Error: 0.2163\n",
      "Epoch:  14 | Loss: 0.4432 | Train Error: 0.2197\n",
      "Epoch:  15 | Loss: 0.4423 | Train Error: 0.2167\n",
      "Epoch:  16 | Loss: 0.4431 | Train Error: 0.2173\n",
      "Epoch:  17 | Loss: 0.4432 | Train Error: 0.2159\n",
      "Epoch:  18 | Loss: 0.4427 | Train Error: 0.2140\n",
      "Epoch:  19 | Loss: 0.4426 | Train Error: 0.2126\n",
      "Epoch:  20 | Loss: 0.4429 | Train Error: 0.2155\n",
      "Epoch:  21 | Loss: 0.4428 | Train Error: 0.2175\n",
      "Epoch:  22 | Loss: 0.4418 | Train Error: 0.2138\n",
      "Epoch:  23 | Loss: 0.4418 | Train Error: 0.2154\n",
      "Epoch:  24 | Loss: 0.4424 | Train Error: 0.2155\n",
      "Epoch:  25 | Loss: 0.4415 | Train Error: 0.2138\n",
      "Epoch:  26 | Loss: 0.4423 | Train Error: 0.2167\n",
      "Epoch:  27 | Loss: 0.4423 | Train Error: 0.2150\n",
      "Epoch:  28 | Loss: 0.4416 | Train Error: 0.2152\n",
      "Epoch:  29 | Loss: 0.4412 | Train Error: 0.2161\n",
      "Epoch:  30 | Loss: 0.4420 | Train Error: 0.2126\n",
      "Epoch:  31 | Loss: 0.4413 | Train Error: 0.2150\n",
      "Epoch:  32 | Loss: 0.4416 | Train Error: 0.2171\n",
      "Epoch:  33 | Loss: 0.4413 | Train Error: 0.2122\n",
      "Epoch:  34 | Loss: 0.4417 | Train Error: 0.2130\n",
      "Epoch:  35 | Loss: 0.4419 | Train Error: 0.2138\n",
      "Epoch:  36 | Loss: 0.4415 | Train Error: 0.2154\n",
      "Epoch:  37 | Loss: 0.4411 | Train Error: 0.2154\n",
      "Epoch:  38 | Loss: 0.4410 | Train Error: 0.2130\n",
      "Epoch:  39 | Loss: 0.4417 | Train Error: 0.2159\n",
      "Epoch:  40 | Loss: 0.4413 | Train Error: 0.2130\n",
      "Epoch:  41 | Loss: 0.4414 | Train Error: 0.2144\n",
      "Epoch:  42 | Loss: 0.4416 | Train Error: 0.2142\n",
      "Epoch:  43 | Loss: 0.4410 | Train Error: 0.2142\n",
      "Epoch:  44 | Loss: 0.4424 | Train Error: 0.2163\n",
      "Epoch:  45 | Loss: 0.4407 | Train Error: 0.2138\n",
      "Epoch:  46 | Loss: 0.4411 | Train Error: 0.2155\n",
      "Epoch:  47 | Loss: 0.4413 | Train Error: 0.2130\n",
      "Epoch:  48 | Loss: 0.4412 | Train Error: 0.2150\n",
      "Epoch:  49 | Loss: 0.4416 | Train Error: 0.2144\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "    train_running_loss = 0.0\n",
    "    train_err = 0.0\n",
    "    model.train()\n",
    "\n",
    "    # TRAINING ROUND\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # get the inputs\n",
    "        inputs, labels = data.values()\n",
    "        inputs = inputs.view(BATCH_SIZE, 3, 144, 256)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # forward + backward + optimize\n",
    "#         outputs = model(inputs.view(BATCH_SIZE, 3, 144, 256),)\n",
    "        outputs = model(inputs,)\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_running_loss += loss.detach().item()\n",
    "        train_err += get_error(outputs.detach(), labels, BATCH_SIZE)\n",
    "\n",
    "    model.eval()\n",
    "    print(\n",
    "        \"Epoch:  %d | Loss: %.4f | Train Error: %.4f\"\n",
    "        % (epoch, train_running_loss / i, train_err / i)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2130c8d0-f222-460f-ac90-a349ec7821b2",
   "metadata": {},
   "source": [
    "### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a279e1a-cb25-4280-b3af-5ab5553cc4da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-17T17:12:57.190005Z",
     "iopub.status.busy": "2021-11-17T17:12:57.189477Z",
     "iopub.status.idle": "2021-11-17T17:12:58.669140Z",
     "shell.execute_reply": "2021-11-17T17:12:58.667617Z",
     "shell.execute_reply.started": "2021-11-17T17:12:57.189963Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rishabh\\anaconda3\\envs\\deeplearn_course\\lib\\site-packages\\ipykernel_launcher.py:48: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking arugment for argument mat2 in method wrapper_mm)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8504/1364095661.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m144\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#     print((outputs.detach() > 0.5).float(), labels)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearn_course\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8504/2701538390.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, img, pos)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;31m# positional embedding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0mimg_patches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearnable_patches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_patches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m         \u001b[0mimg_patches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_cls_to_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_patches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearn_course\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearn_course\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\deeplearn_course\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1845\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1847\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1848\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking arugment for argument mat2 in method wrapper_mm)"
     ]
    }
   ],
   "source": [
    "test_err = 0.0\n",
    "for i, data in enumerate(valid_loader, 0):\n",
    "    inputs, labels = data.values()\n",
    "\n",
    "    outputs = model(inputs.view(BATCH_SIZE, 3, 144, 256).to(device))\n",
    "    outputs = torch.sigmoid(outputs)\n",
    "    #     print((outputs.detach() > 0.5).float(), labels)\n",
    "\n",
    "    test_err += get_error(outputs.detach(), labels, BATCH_SIZE)\n",
    "\n",
    "print(\"Test Error: %.4f\" % (test_err / i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c7ca70",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
